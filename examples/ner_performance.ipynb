{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kashgari NER Benchmarks\n",
    "\n",
    "- Kashgari: 2.1.0\n",
    "- TensorFLow: 2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Embeddings to Embddings Folder and unzip.\n",
    "- [BERT-Base, Chinese](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)\n",
    "\n",
    "Final folder struct is\n",
    "\n",
    "```\n",
    ".\n",
    "└── embeddings\n",
    "    └── chinese_L-12_H-768_A-12\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_ON_CLOUD = False\n",
    "\n",
    "if RUNNING_ON_CLOUD:\n",
    "    !mkdir embeddings\n",
    "    !wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip embeddings/chinese_L-12_H-768_A-12.zip\n",
    "    !unzip embeddings/chinese_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup macros\n",
    "if RUNNING_ON_CLOUD:\n",
    "    EMBEDDING_FOLDER = './embeddings'\n",
    "    TF_LOG_FOLDER = './tf_dir'\n",
    "    LOG_FILE_PATH = './ner_training_log.json'\n",
    "else:\n",
    "    EMBEDDING_FOLDER = '/Users/brikerman/Desktop/kashgari-demo/embeddings'\n",
    "    TF_LOG_FOLDER = 'tf_dir'\n",
    "    LOG_FILE_PATH = 'ner_training_log.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kashgari.corpus import ChineseDailyNerCorpus\n",
    "\n",
    "train_x, train_y = ChineseDailyNerCorpus.load_data('train')\n",
    "test_x, test_y = ChineseDailyNerCorpus.load_data('test')\n",
    "valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid')\n",
    "\n",
    "train_x = train_x[:200]\n",
    "train_y = train_y[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tensorflow import keras\n",
    "from kashgari.tasks.labeling import BiLSTM_Model, BiLSTM_CRF_Model\n",
    "from kashgari.tasks.labeling import BiGRU_Model, BiGRU_CRF_Model\n",
    "from kashgari.callbacks import EvalCallBack\n",
    "\n",
    "from kashgari.embeddings import WordEmbedding, BertEmbedding\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_chinese = BertEmbedding(os.path.join(EMBEDDING_FOLDER, 'chinese_L-12_H-768_A-12'))\n",
    "embeddings = [\n",
    "    # ('Bert-Chinese', bert_chinese),\n",
    "    ('Bare', None)\n",
    "]\n",
    "\n",
    "model_classes = [\n",
    "    ('BiLSTM', BiLSTM_Model), \n",
    "    ('BiLSTM_CRF', BiLSTM_CRF_Model), \n",
    "    ('BiGRU', BiGRU_Model), \n",
    "    ('BiGRU_CRF', BiGRU_CRF_Model)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Preparing text vocab dict: 100%|██████████| 200/200 [00:00<00:00, 66444.42it/s]\nPreparing text vocab dict: 100%|██████████| 200/200 [00:00<00:00, 95422.68it/s]\nCalculating sequence length: 100%|██████████| 200/200 [00:00<00:00, 584979.64it/s]\nWARNING:root:Calculated sequence length = 92\nModel: \"model_17\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput (InputLayer)           [(None, None)]            0         \n_________________________________________________________________\nlayer_embedding (Embedding)  (None, None, 100)         69900     \n_________________________________________________________________\nlayer_blstm (Bidirectional)  (None, None, 256)         234496    \n_________________________________________________________________\nlayer_dropout (Dropout)      (None, None, 256)         0         \n_________________________________________________________________\ndense_6 (Dense)              (None, None, 8)           2056      \n_________________________________________________________________\nactivation_4 (Activation)    (None, None, 8)           0         \n=================================================================\nTotal params: 306,452\nTrainable params: 306,452\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain for 3 steps, validate for 36 steps\n2/3 [===================>..........] - ETA: 2s - loss: 2.0599 - accuracy: 0.3291WARNING:root:Sequence length is None, will use the max length of the samples, which is 568\n           precision    recall  f1-score   support\n\n      LOC     0.0000    0.0000    0.0000      1951\n      PER     0.0000    0.0000    0.0000       884\n      ORG     0.0000    0.0000    0.0000       984\n\nmicro avg     0.0000    0.0000    0.0000      3819\nmacro avg     0.0000    0.0000    0.0000      3819\n\n\nepoch: 0 precision: 0.000000, recall: 0.000000, f1-score: 0.000000\n3/3 [==============================] - 21s 7s/step - loss: 2.0324 - accuracy: 0.4419 - val_loss: 1.9152 - val_accuracy: 0.7399\nPreparing text vocab dict: 100%|██████████| 200/200 [00:00<00:00, 87710.25it/s]\nPreparing text vocab dict: 100%|██████████| 200/200 [00:00<00:00, 89516.68it/s]\nCalculating sequence length: 100%|██████████| 200/200 [00:00<00:00, 243925.79it/s]\nWARNING:root:Calculated sequence length = 92\nModel: \"model_19\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput (InputLayer)           [(None, None)]            0         \n_________________________________________________________________\nlayer_embedding (Embedding)  (None, None, 100)         69900     \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, None, 256)         234496    \n_________________________________________________________________\nlayer_dropout (Dropout)      (None, None, 256)         0         \n_________________________________________________________________\ndense_7 (Dense)              (None, None, 8)           2056      \n_________________________________________________________________\nlayer_crf (ConditionalRandom (None, None, 8)           0         \n=================================================================\nTotal params: 306,452\nTrainable params: 306,452\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain for 3 steps, validate for 36 steps\n2/3 [===================>..........] - ETA: 2s - loss: 201.3608 - dense_accuracy: 0.3521WARNING:root:Sequence length is None, will use the max length of the samples, which is 568\n           precision    recall  f1-score   support\n\n      LOC     0.0000    0.0000    0.0000      1951\n      PER     0.0000    0.0000    0.0000       884\n      ORG     0.0000    0.0000    0.0000       984\n\nmicro avg     0.0000    0.0000    0.0000      3819\nmacro avg     0.0000    0.0000    0.0000      3819\n\n\nepoch: 0 precision: 0.000000, recall: 0.000000, f1-score: 0.000000\n3/3 [==============================] - 23s 8s/step - loss: 198.8887 - dense_accuracy: 0.4609 - val_loss: 187.0614 - val_dense_accuracy: 0.7490\nPreparing text vocab dict: 100%|██████████| 200/200 [00:00<00:00, 76657.30it/s]\nPreparing text vocab dict: 100%|██████████| 200/200 [00:00<00:00, 97849.15it/s]\nCalculating sequence length: 100%|██████████| 200/200 [00:00<00:00, 156358.02it/s]\nWARNING:root:Calculated sequence length = 92\nModel: \"model_21\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput (InputLayer)           [(None, None)]            0         \n_________________________________________________________________\nlayer_embedding (Embedding)  (None, None, 100)         69900     \n_________________________________________________________________\nlayer_bgru (Bidirectional)   (None, None, 256)         176640    \n_________________________________________________________________\nlayer_dropout (Dropout)      (None, None, 256)         0         \n_________________________________________________________________\nlayer_time_distributed (Time (None, None, 8)           2056      \n_________________________________________________________________\nactivation_5 (Activation)    (None, None, 8)           0         \n=================================================================\nTotal params: 248,596\nTrainable params: 248,596\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain for 3 steps, validate for 36 steps\n2/3 [===================>..........] - ETA: 1s - loss: 2.0568 - accuracy: 0.3404WARNING:root:Sequence length is None, will use the max length of the samples, which is 568\n           precision    recall  f1-score   support\n\n      LOC     0.0000    0.0000    0.0000      1951\n      PER     0.0000    0.0000    0.0000       884\n      ORG     0.0000    0.0000    0.0000       984\n\nmicro avg     0.0000    0.0000    0.0000      3819\nmacro avg     0.0000    0.0000    0.0000      3819\n\n\nepoch: 0 precision: 0.000000, recall: 0.000000, f1-score: 0.000000\n3/3 [==============================] - 17s 6s/step - loss: 2.0296 - accuracy: 0.4898 - val_loss: 1.9180 - val_accuracy: 0.8714\nPreparing text vocab dict: 100%|██████████| 200/200 [00:00<00:00, 56322.06it/s]\nPreparing text vocab dict: 100%|██████████| 200/200 [00:00<00:00, 52271.98it/s]\nCalculating sequence length: 100%|██████████| 200/200 [00:00<00:00, 255050.41it/s]\nWARNING:root:Calculated sequence length = 92\nModel: \"model_23\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput (InputLayer)           [(None, None)]            0         \n_________________________________________________________________\nlayer_embedding (Embedding)  (None, None, 100)         69900     \n_________________________________________________________________\nlayer_gru (Bidirectional)    (None, None, 256)         176640    \n_________________________________________________________________\nlayer_dropout (Dropout)      (None, None, 256)         0         \n_________________________________________________________________\nlayer_dense (Dense)          (None, None, 64)          16448     \n_________________________________________________________________\nlayer_crf_dense (Dense)      (None, None, 8)           520       \n_________________________________________________________________\nlayer_crf (ConditionalRandom (None, None, 8)           0         \n=================================================================\nTotal params: 263,508\nTrainable params: 263,508\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain for 3 steps, validate for 36 steps\n2/3 [===================>..........] - ETA: 2s - loss: 219.6397 - dense_accuracy: 0.3811WARNING:root:Sequence length is None, will use the max length of the samples, which is 568\n           precision    recall  f1-score   support\n\n      LOC     0.0000    0.0000    0.0000      1951\n      PER     0.0000    0.0000    0.0000       884\n      ORG     0.0000    0.0000    0.0000       984\n\nmicro avg     0.0000    0.0000    0.0000      3819\nmacro avg     0.0000    0.0000    0.0000      3819\n\n\nepoch: 0 precision: 0.000000, recall: 0.000000, f1-score: 0.000000\n3/3 [==============================] - 20s 7s/step - loss: 216.1849 - dense_accuracy: 0.5228 - val_loss: 203.3964 - val_dense_accuracy: 0.8930\n"
    }
   ],
   "source": [
    "for embed_name, embed in embeddings:\n",
    "    for model_name, MOEDL_CLASS in model_classes:\n",
    "        run_name = f\"{embed_name}-{model_name}\"\n",
    "        model = MOEDL_CLASS(embed)\n",
    "        \n",
    "        early_stop = keras.callbacks.EarlyStopping(patience=5)\n",
    "        reduse_lr_callback = keras.callbacks.ReduceLROnPlateau(factor=0.1, \n",
    "                                                               patience=5)\n",
    "\n",
    "        eval_callback = EvalCallBack(task_model=model,\n",
    "                                     valid_x=valid_x, \n",
    "                                     valid_y=valid_y,\n",
    "                                     step=1)\n",
    "\n",
    "        tf_board = keras.callbacks.TensorBoard(\n",
    "            log_dir=os.path.join(TF_LOG_FOLDER, run_name), \n",
    "            update_freq=1000\n",
    "        )\n",
    "\n",
    "        callbacks = [early_stop, reduse_lr_callback, eval_callback, tf_board]\n",
    "\n",
    "        model.fit(train_x, \n",
    "                  train_y,\n",
    "                  valid_x,\n",
    "                  valid_y,\n",
    "                  callbacks=callbacks,\n",
    "                  epochs=1)\n",
    "\n",
    "        if os.path.exists(LOG_FILE_PATH):\n",
    "            logs = json.load(open(LOG_FILE_PATH, 'r'))\n",
    "        else:\n",
    "            logs = {}\n",
    "        \n",
    "        logs[run_name] = eval_callback.logs\n",
    "        \n",
    "        with open(LOG_FILE_PATH, 'w') as f:\n",
    "            f.write(json.dumps(logs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_x[:200], train_y[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(train_x[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37464bitvenvvenv1fddca7c786445709a8f03a12aa91dfc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}